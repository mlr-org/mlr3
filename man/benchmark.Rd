% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/benchmark.R
\name{benchmark}
\alias{benchmark}
\title{Benchmark Multiple Learners on Multiple Tasks}
\usage{
benchmark(tasks, learners, resamplings, measures = NULL,
  ctrl = exec_control())
}
\arguments{
\item{tasks}{[\code{list} of \link{Task}]:\cr
List of objects of type \link{Task}.}

\item{learners}{[\code{list} of \link{Learner}]:\cr
List of objects of type \link{Learner}.}

\item{resamplings}{[\code{list} of \link{Resampling}]:\cr
List of objects of type \link{Resampling}.}

\item{measures}{[\code{list} of \link{Measure}]:\cr
List of performance measures used to assess the predictive performance.
Defaults to the respective measures stored in \code{task}.}

\item{ctrl}{[\code{named list} as returned by \code{\link[=exec_control]{exec_control()}}]:\cr
Object to control experiment execution. See \code{\link[=exec_control]{exec_control()}}.}
}
\value{
\link{BenchmarkResult}.
}
\description{
Runs a benchmark of the cross-product of learners, tasks, and resampling strategies (possibly in parallel).
}
\examples{
tasks = mlr_tasks$mget(c("iris", "sonar"))
learners = mlr_learners$mget(c("classif.dummy", "classif.rpart"))
resamplings = mlr_resamplings$mget(c("holdout", "cv"))
measures = mlr_measures$mget(c("acc", "time_train"))
bmr = benchmark(tasks, learners, resamplings, measures)

# performance for all conducted experiments
bmr$performance

# aggregated performance
bmr$aggregated

# Overview of of resamplings that were conducted internally
rrs = bmr$resample_results
print(rrs)

# Extract first ResampleResult
rr = bmr$resample_result(hash = rrs$hash[1])
print(rr)

# Extract predictions of first experiment of this resampling
rr$experiment(1)$prediction
}
