confusion_measures = setkeyv(dribble(
  ~id,           ~lower, ~upper, ~minimize, ~na_score,
  "tp",          0,      Inf,    FALSE,     FALSE,
  "fn",          0,      Inf,    TRUE,      FALSE,
  "fp",          0,      Inf,    TRUE,      FALSE,
  "tn",          0,      Inf,    FALSE,     FALSE,
  "tpr",         0,      1,      FALSE,     TRUE,
  "fnr",         0,      1,      TRUE,      TRUE,
  "fpr",         0,      1,      TRUE,      TRUE,
  "tnr",         0,      1,      FALSE,     TRUE,
  "ppv",         0,      1,      FALSE,     TRUE,
  "fdr",         0,      1,      TRUE,      TRUE,
  "for",         0,      1,      TRUE,      TRUE,
  "npv",         0,      1,      FALSE,     TRUE,
  "precision",   0,      1,      FALSE,     TRUE,
  "recall",      0,      1,      FALSE,     TRUE,
  "sensitivity", 0,      1,      FALSE,     TRUE,
  "specificity", 0,      1,      FALSE,     TRUE
), "id")

#' @title Binary Classification Measures Derived from a Confusion Matrix
#'
#' @name mlr_measures_classif.confusion
#' @format [R6::R6Class()] inheriting from [MeasureClassif].
#' @include MeasureClassif.R
#'
#' @description
#' Based on a confusion matrix for binary classification problems, allows to calculate various performance measures.
#' Implemented are the following measures based on \url{https://en.wikipedia.org/wiki/Template:DiagnosticTesting_Diagram}:
#'
#' * `"tp"`: True Positives.
#' * `"fn"`: False Negatives.
#' * `"fp"`: False Positives.
#' * `"tn"`: True Negatives.
#' * `"tpr"`: True Positive Rate.
#' * `"fnr"`: False Negative Rate.
#' * `"fpr"`: False Positive Rate.
#' * `"tnr"`: True Negative Rate.
#' * `"ppv"`: Positive Predictive Value.
#' * `"fdr"`: False Discovery Rate.
#' * `"for"`: False Omission Rate.
#' * `"npv"`: Negative Predictive Value.
#' * `"precision"`: Alias for `"ppv"`.
#' * `"recall"`: Alias for `"tpr"`.
#' * `"sensitivity"`: Alias for `"tpr"`.
#' * `"specificity"`: Alias for `"tnr"`.
#'
#' If the denominator is 0, the score is returned as `NA`.
#'
#' @aliases
#'  mlr_measures_classif.tp
#'  mlr_measures_classif.fn
#'  mlr_measures_classif.fp
#'  mlr_measures_classif.tn
#'  mlr_measures_classif.tpr
#'  mlr_measures_classif.fnr
#'  mlr_measures_classif.fpr
#'  mlr_measures_classif.tnr
#'  mlr_measures_classif.ppv
#'  mlr_measures_classif.fdr
#'  mlr_measures_classif.for
#'  mlr_measures_classif.npv
#'  mlr_measures_classif.precision
#'  mlr_measures_classif.recall
#'  mlr_measures_classif.sensitivity
#'  mlr_measures_classif.specificity
#' @export
MeasureClassifConfusion = R6Class("MeasureClassifConfusion",
  inherit = MeasureClassif,
  cloneable = FALSE,
  public = list(
    type = NULL,
    initialize = function(type) {
      self$type = assert_choice(type, confusion_measures$id)
      row = as.list(confusion_measures[list(type)])

      super$initialize(
        id = sprintf("classif.%s", type),
        range = c(row$lower, row$upper),
        minimize = row$minimize,
        predict_type = "response",
        task_properties = "twoclass",
        na_score = row$na_score
      )
    },

    calculate = function(e) {
      extract_from_confusion(e$prediction$confusion, self$type)
    }
  )
)

#' @rdname mlr_measures_classif.confusion
#'
#' @param m (`matrix()`)\cr
#'   Confusion matrix, e.g. as returned by field `confusion` of [PredictionClassif].
#'   Truth is in columns, predicted response is in rows.
#' @param type (`character(1)`)\cr
#'   Selects the measure to use. See description.
#'
#' @export
extract_from_confusion = function(m, type) {
  div = function(nominator, denominator) {
    if (denominator == 0L)
      return(NA_real_)
    nominator / denominator
  }

  switch(type,
    "tp" = m[1L, 1L],
    "fn" = m[2L, 1L],
    "fp" = m[1L, 2L],
    "tn" = m[2L, 2L],
    "tpr" = , "recall" = , "sensitivity" = div(m[1L, 1L], sum(m[, 1L])),
    "fnr" = div(m[2L, 1L], sum(m[, 1L])),
    "fpr" = div(m[1L, 2L], sum(m[, 2L])),
    "tnr" = , "specificity" = div(m[2L, 2L], sum(m[, 2L])),
    "ppv" = , "precision" = div(m[1L, 1L], sum(m[1L, ])),
    "fdr" = div(m[1L, 2L], sum(m[1L, ])),
    "for" = div(m[2L, 1L], sum(m[2L, ])),
    "npv" = div(m[2L, 2L], sum(m[2L, ])),
    stop("Unknown measure to extract from confusion matrix")
  )
}


for (type in confusion_measures$id)
  mlr_measures$add(sprintf("classif.%s", type), MeasureClassifConfusion$new(type))
