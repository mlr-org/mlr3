---
title: "Introduction to Resampling"
author: "Michel Lang"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to Resampling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(mlr3)
knitr::opts_knit$set(
  datatable.print.keys = FALSE,
  datatable.print.class = TRUE
)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
set.seed(123)
```

This introduction is about resampling.

## Objects

Again, we consider the iris task and a simple classification tree here.

```{r}
task = mlr_tasks$get("iris")
learner = mlr_learners$get("classif.rpart")
```

Additionally, we need to define **how** we want to resample. `mlr3` comes with the following resampling strategies implemented:

```{r}
mlr_resamplings$keys()
```
The experiment conducted in the introduction on train/predict/score is equivalent to a simple "holdout", so let's consider this one first.

```{r}
resampling = mlr_resamplings$get("holdout")
print(resampling)
print(resampling$par_set)
print(resampling$par_vals)
```
To change the ratio to $0.8$, we simply overwrite the slot:

```{r}
resampling$par_vals = list(ratio = 0.8)
```

## Resampling

Now, we can pass all created objects to the `resample()` function to get an object of class `ResampleResult`:

```{r}
rr = resample(task, learner, resampling)
print(rr)
```

Before we go into more detail, lets change the resampling to a 3-fold cross-validation to better illustrate what operations are possible with a resampling result.

```{r}
resampling = mlr_resamplings$get("cv")
resampling$par_vals = list(folds = 3)
rr = resample(task, learner, resampling)
print(rr)
```

We can do different things with resampling results, e.g.:

* Extract the performance per fold and average it:

```{r}
rr$performance
rr$performance[, mean(mmce)]
```

* Extract and inspect the now instantiated resampling:

```{r}
rr$resampling
rr$resampling$iters
rr$resampling$test_set(1)
rr$resampling$test_set(2)
rr$resampling$test_set(3)
```

* Retrieve the experiment of a specific iteration and inspect it:

```{r}
e = rr$experiment(iter = 1)
e$model
```

## Manual Instantiation

If you want to compare multiple learners, you should use the same resampling per task to reduce the variance of the performance estimation.
Until now, we have just passed a resampling strategy to `resample()`, without specifying the actual splits into training and test.
Here, we manually instantiate the resampling:

```{r}
resampling = mlr_resamplings$get("cv")
resampling$par_vals = list(folds = 3)
resampling$instantiate(task)
resampling$iters
resampling$train_set(1)
```
If we now pass this instantiated object to resample, the pre-calculated training and test splits will be used for both learners:

```{r}
learner1 = mlr_learners$get("classif.rpart")
learner2 = mlr_learners$get("classif.dummy")
rr1 = resample(task, learner1, resampling)
rr2 = resample(task, learner2, resampling)

setequal(rr1$experiment(1)$train_set, rr2$experiment(1)$train_set)
```
We can also combine the created result objects into a `BenchmarkResult`:

```{r}
bmr = rr1$combine(rr2)
bmr$performance
```

## Custom Resampling

Sometimes it is necessary to perform resampling with custom splits, e.g. to reproduce a study.
For this purpose, splits can be manually set for `ResamplingCustom`:

```{r}
resampling = mlr_resamplings$get("custom")
resampling$instantiate(task,
  list(c(1:10, 51:60, 101:110)),
  list(c(11:20, 61:70, 111:120))
)
resampling$iters
resampling$train_set(1)
resampling$test_set(1)
```
