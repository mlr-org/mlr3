---
title: "Handling model errors"
author: "Michel Lang"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Handling model errors}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
library(mlr3)
knitr::opts_knit$set(
  datatable.print.keys = FALSE,
  datatable.print.class = TRUE
)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
future::plan("sequential")
set.seed(123)

### https://stackoverflow.com/questions/23442249/get-traceback-from-knitr-on-error
saveTraceback <- local({
  savedTraceback <- NULL
  saver <- function(e) {
    calls <- sys.calls()
    deparsed <- lapply(calls, deparse)
    deparsed <- deparsed[-length(deparsed)+0:1] # leave off last 2
    lastjunk <- max(grep("withCallingHandlers", deparsed))
    deparsed <- deparsed[-seq_len(lastjunk)]
    savedTraceback <<- deparsed
  }
  function(expr)
    withCallingHandlers(expr, error = saver)
})

traceback <- function() {
  base::traceback(environment(saveTraceback)$savedTraceback)
}
```

This vignettes demonstrates how to deal with learners which raise exceptions during train or predict.

# Setup

First, we need a simple learning task and a learner which raises exceptions.
For this purpose, `mlr3` ships with the learner `classif.crashtest`:
```{r}
task = mlr_tasks$get("spam")
learner = mlr_learners$get("classif.crashtest")
print(learner)
print(learner$param_set)
```
The hyperparameters let us control (a) whether it should crash during train or predict, and (b) if the learner should crash with an error or segfault (which tears down the complete R session).


# No Error Handling

In the defaults, `mlr3` does not handle model errors. Thus, the crashtest learner raises a normal exception which we can traceback:

```{r, error=TRUE, echo = -3}
e = Experiment$new(task, learner)
e$train()
safeTraceback(e$train())
traceback()
```
The traceback might be obfuscated by the parallelization framework, depending on your installation.
To get a better traceback, we turn the parallelization completely off:
```{r, error = TRUE, echo = -3}
ctrl = mlr_control(use_future = FALSE)
e$train(ctrl = ctrl)
safeTraceback(e$train(ctrl = ctrl))
traceback()
```

# Catching and Logging

We can tell `mlr3` to catch errors for us and process computation.
In this mode, we can proceed with the computation of the experiment stages (train, predict, score) even if there are errors in the stages.
However, the resulting performance will be `NA` if errors occurred.
This mode is especially useful for `resample()` or `benchmark()`, but we demonstrate it here for a single `Experiment`:
```{r}
ctrl = mlr_control(error_handling = "catch")
e$train(ctrl = ctrl)
print(e)
e$state
```
No exception has been raised, and the experiment has state `"trained"`.
We can have a look at the output which has been captured and ask if there was an error:
```{r}
e$logs$train
e$logs$train$has_condition("error")
```
Although there has been an error, we can proceed to work with this `Experiment`:
```{r}
e$predict(ctrl = ctrl)$score(ctrl = ctrl)
e$state
```
We cannot access the predictions (as there are none), but we can access the performance slot:
```{r, error = TRUE}
e$prediction
e$performance
```

# Impute performance

Generating `NA` scores and ignoring them during the aggregation can lead to overoptimism in the results.

For example, consider a 10-fold CV on a small task where half of the splits are easy to learn, yielding good performance values (accuracy 0.9), and the other half is rather difficult (accuracy = 0.6).

Learner A successfully learns models for all splits, and has an average accuracy of $(5 * 0.9 + 5 * 0.6) / 10 = 0.75$.
Learner B simply raises an exception for the difficult splits, and after removing the missing values for averaging we get an accuracy of $(5 * 0.9) / 5 = 0.9$.
We don't know the real accuracy for learner B, but 0.9 is obviously not a good estimate.

As a way out, `mlr3` can set the performance of a measure to the worst possible value (instead of `NA`) if the learner is unable to generate predictions:
```{r}
ctrl = mlr_control(error_handling = "impute_worst")
e$train(ctrl = ctrl)$predict(ctrl = ctrl)$score(ctrl = ctrl)
e$performance
```

# Fallback Learners

Instead of penalizing the learner with the worst possible performance, it is sometimes (e.g., while tuning) more reasonable to fallback to a simple, but more robust learner.
Here, we simply fallback to the predictions of a featureless learner (predicting majority class):

```{r}
ctrl = mlr_control(error_handling = "fallback", fallback_learner = mlr_learners$get("classif.featureless"))
e$train(ctrl = ctrl)$predict(ctrl = ctrl)$score(ctrl = ctrl)
e$prediction
e$performance
```

Note that the logs and timings are tracked for the original learner, not the fallback learner:
```{r}
e$logs
```

